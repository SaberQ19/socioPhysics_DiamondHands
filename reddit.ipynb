{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit submissions and comments\n",
    "This notebook shows how to read the Reddit dataset and how to compare the content of the files with the Reddit webpages. We will learn how to select the relevant information from the database and make some simple analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read and handle a json file\n",
    "We first need to open the json file containing all the reddit submissions related to a certain stock; here we focus on FIZZ. We start by getting the data from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sociophysicsDataHandler import SociophysicsDataHandler\n",
    "\n",
    "student_config = True\n",
    "\n",
    "file_target = 'asdz/platform2.2/20200428/ASDZ_Perron2.2_2020042815_trajectorie.parquet' \n",
    "\n",
    "if student_config:\n",
    "    dh = SociophysicsDataHandler()\n",
    "    dh.fetch_prorail_data_from_path(file_target)\n",
    "else:\n",
    "    webdav_basepath='/Crowdflow (Projectfolder)/ProRail_USE_LL_data'\n",
    "    dh = SociophysicsDataHandler(basepath=webdav_basepath)\n",
    "    \n",
    "    dh.fetch_prorail_data_from_path(file_target)\n",
    "                           # ,basepath=webdav_basepath)\n",
    "\n",
    "print('The available files are the following:')\n",
    "dh.list_files(\"econophysics/reddit/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stock = 'FIZZ'\n",
    "filename = 'submissions_wallstreetbets_' + stock + '_start20200901_end20210706.json' # insert here your path \n",
    "dh.fetch_econophysics_data_from_path(\"econophysics/reddit/\" + filename)\n",
    "df = dh.df\n",
    "# print one of the entries (in this case, the fifth):\n",
    "print(df.iloc[170])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with the corresponding Reddit webpage\n",
    "Let's consider for example the submission identified by the code 'o65l9k'. We can understand what the field contained in the json files mean by looking at the corresponding Reddit webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = 'o65l9k'\n",
    "df.loc[subm]\n",
    "print('')\n",
    "print('The web link of submission', subm, 'is: ', df['full_link'].loc[subm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the comments associated to the submission are in a separate json file:\n",
    "dh.fetch_econophysics_data_from_path(\"econophysics/reddit/comments_\" + stock + \".tar.gz\")\n",
    "dh.reddit_comments.get_comment_matching_id(subm)\n",
    "df_comments = dh.reddit_comments.df[0]\n",
    "df_comments.head()\n",
    "\n",
    "#dh.reddit_comments.get_file_names() # to get the list files in the tar archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_comments = len(df_comments)\n",
    "print('Number of comments:', num_comments)\n",
    "print('For example, a comment is:')\n",
    "df_comments.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can check the hierarchy of comments and replies by looking at the 'parent_id' field:\n",
    "df_comments[['id','parent_id','body']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the relevant information\n",
    "Not all the fields are equally useful for our analysis; here we select only some of them (this list is not exhaustive) in the case of the submissions file. A similar subsection can be made for the comments files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fields = ['author_fullname','created_utc','num_comments','permalink','score','title','upvote_ratio']\n",
    "df = df[list_fields]\n",
    "print('The shape of the dataframe is now ', df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the times into readable format\n",
    "One of the crucial features of the Reddit database is the time at which the submissions and comments have been made. In the json files, times are saved as integer values. Here we transform these values into DateTime values; the times are reported in GMT values (Greenwich Mean Time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['created_utc'] = pd.to_datetime(df['created_utc'], origin='unix', unit='s') \n",
    "# created_utc is the time when the submission was created by its author\n",
    "print(df[['author_fullname','created_utc']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a simple analysis (1)\n",
    "Here we begin the analysis of the submissions, checking the distribution of the number of comments associated to each post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "num_bins = 40 # number of bins for the histogram\n",
    "pl.hist(df['num_comments'].values, bins=num_bins)\n",
    "pl.xlabel('Number of comments', fontsize=14)\n",
    "pl.ylabel('Frequency', fontsize=14)\n",
    "pl.title('Comments distribution, FIZZ', fontsize=14)\n",
    "pl.show()\n",
    "\n",
    "print('The maximum number of comments associated to a submission is', df['num_comments'].max())\n",
    "id_max = df[df['num_comments'] == df['num_comments'].max()].index[0]\n",
    "print('The most commented post about FIZZ is identified by', id_max)\n",
    "print('Its title is \"', df['title'].loc[id_max], '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a simple analysis (2)\n",
    "We can also sort our dataframe based on some field. For example, here we rank the submissions based on their score and check what the correlation is between number of comments of submission score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort in descending order, from the highest score to the lowest\n",
    "df_sorted = df.sort_values(['score'], ascending=False) \n",
    "print(df_sorted.head())\n",
    "\n",
    "# compute correlation between two columns of the dataframe:\n",
    "correlation = df['score'].corr(df['num_comments'])\n",
    "print('The correlation between score and number of comments is', correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

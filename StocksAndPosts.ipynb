{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sociophysicsDataHandler import SociophysicsDataHandler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "student_config = True\n",
    "\n",
    "file_target = 'asdz/platform2.2/20200428/ASDZ_Perron2.2_2020042815_trajectorie.parquet' \n",
    "\n",
    "if student_config:\n",
    "    dh = SociophysicsDataHandler()\n",
    "    dh.fetch_prorail_data_from_path(file_target)\n",
    "else:\n",
    "    webdav_basepath='/Crowdflow (Projectfolder)/ProRail_USE_LL_data'\n",
    "    dh = SociophysicsDataHandler(basepath=webdav_basepath)\n",
    "    \n",
    "    dh.fetch_prorail_data_from_path(file_target)\n",
    "                           # ,basepath=webdav_basepath)\n",
    "\n",
    "print('The available files are the following:')\n",
    "dh.list_files(\"econophysics/reddit/\")\n",
    "for path in dh.filelist['path']:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetPosts = True\n",
    "stock = 'GME'\n",
    "\n",
    "if targetPosts:\n",
    "    filename = 'submissions_wallstreetbets_' + stock + '_start20200901_end20210706.json'\n",
    "    if stock == 'GME':\n",
    "        filename = 'submissions_wallstreetbets_GME_start20200901_end20210624.json'\n",
    "else:\n",
    "    filename = 'comments_' + stock + '.tar.gz'\n",
    "    \n",
    "dh.fetch_econophysics_data_from_path(\"econophysics/reddit/\" + filename)\n",
    "df = dh.df\n",
    "# print one of the entries (in this case, the fifth):\n",
    "df\n",
    "\n",
    "for col in df.columns:\n",
    "    print(col)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create time sorted reddit post df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditPosts = df.set_index('created_utc')\n",
    "redditPosts.sort_index(inplace=True)\n",
    "redditPosts.index = pd.to_datetime(redditPosts.index, origin='unix', unit='s')\n",
    "\n",
    "redditPosts\n",
    "#This is now sorted on time\n",
    "\n",
    "#Set rolling window to 5 days, by default this rolling window will take the right-most boundary as center\n",
    "#Pretty aweosme\n",
    "redditRollingAmount = redditPosts[['id']].rolling(window = '1H').count()\n",
    "redditRollingAmount = redditRollingAmount.rename(columns={'id':'rolling_count'})\n",
    "\n",
    "redditRollingAmount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh.fetch_econophysics_data_from_path(\"econophysics/prices/hourly_prices.csv\")\n",
    "prices_hour = dh.df\n",
    "prices_hour.index = pd.to_datetime(prices_hour.index) # to be sure that the index is in the pandas DateTime format\n",
    "\n",
    "# the times you see in the index of prices_hour are expressed in New York time (American Eastern Time) \n",
    "# American Eastern Time is defined as UTC-5 in autumn and winter, and UTC-4 in spring and summer (daylight saving)\n",
    "import datetime as dt\n",
    "from dateutil import tz # library to treat timezones\n",
    "NYC = tz.gettz('America/New_York') # define the New York timezone\n",
    "\n",
    "new_index_list = []\n",
    "for i in range(len(prices_hour)):\n",
    "    # for each index, make the previous index transformation:\n",
    "    old_index = prices_hour.index[i]\n",
    "    dat = str(old_index.date())\n",
    "    dt1 = dt.datetime(int(dat[0:4]), int(dat[5:7]),int(dat[8:10]), tzinfo=NYC)\n",
    "    UTC_lag = dt1.utcoffset() / dt.timedelta(hours=1)\n",
    "    new_index = prices_hour.index[i] - pd.Timedelta(hours=UTC_lag)\n",
    "    # attach the UTC index to the new index list:\n",
    "    new_index_list.append(new_index)\n",
    "    \n",
    "# set the new UTC index to the prices_hour dataframe:\n",
    "prices_hour.index = new_index_list\n",
    "\n",
    "prices_hour #corrected reddit time prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "stock = 'GME'\n",
    "start_month = '2021-01-20'\n",
    "end_month = '2021-01-31'\n",
    "prices_toPlot = prices_hour[[stock]][start_month:end_month]\n",
    "prices_toPlot = prices_toPlot.dropna()\n",
    "\n",
    "redditRollingAmount_toPlot = redditRollingAmount[['rolling_count']][start_month:end_month]\n",
    "\n",
    "fig, axes = pl.subplots(nrows=2, ncols=1, sharex=True)\n",
    "\n",
    "prices_toPlot.plot(ax=axes[0], color='g')\n",
    "axes[0].set(xlabel='b')\n",
    "axes[0].set(ylabel=f'Price of {stock}')\n",
    "axes[0].set(title='GME price and post count comparison')\n",
    "axes[0].get_legend().remove()\n",
    "\n",
    "redditRollingAmount_toPlot.plot(ax=axes[1], color='r')\n",
    "axes[1].set(xlabel='Time')\n",
    "axes[1].set(ylabel=f'Rolling Post Count')\n",
    "axes[1].get_legend().remove()\n",
    "\n",
    "pl.show()\n",
    "\n",
    "prices_toPlot\n",
    "\n",
    "axes[0].axis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_peak_start = '2021-01-01'\n",
    "before_peak_end = '2021-01-22'\n",
    "after_peak_start = '2021-01-23'\n",
    "after_peak_end = '2021-01-30'\n",
    "\n",
    "redditPostsBeforePeak = redditPosts[['link_flair_text']][before_peak_start:before_peak_end].dropna()\n",
    "redditPostsAfterPeak = redditPosts[['link_flair_text']][after_peak_start:after_peak_end].dropna()\n",
    "\n",
    "countFlairsBefore = {}\n",
    "for val in redditPostsBeforePeak['link_flair_text']:\n",
    "    countFlairsBefore[val] = 0\n",
    "    \n",
    "countFlairsBefore = dict(sorted(countFlairsBefore.items()))\n",
    "\n",
    "for r in redditPostsBeforePeak['link_flair_text']:\n",
    "    countFlairsBefore[r] = countFlairsBefore[r] + 1\n",
    "    \n",
    "countFlairsAfter = {}\n",
    "for val in redditPostsAfterPeak['link_flair_text']:\n",
    "    countFlairsAfter[val] = 0\n",
    "    \n",
    "countFlairsAfter = dict(sorted(countFlairsAfter.items()))\n",
    "\n",
    "for r in redditPostsAfterPeak['link_flair_text']:\n",
    "    countFlairsAfter[r] = countFlairsAfter[r] + 1\n",
    "\n",
    "\n",
    "countFlairsBefore = {k: v for k, v in countFlairsBefore.items() if v > 150}\n",
    "countFlairsAfter = {k: v for k, v in countFlairsAfter.items() if v > 800}\n",
    "\n",
    "pl.barh(range(len(countFlairsBefore)), list(countFlairsBefore.values()), tick_label=list(countFlairsBefore.keys()))\n",
    "pl.title('GME post category January 1st till 22nd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.barh(range(len(countFlairsAfter)), list(countFlairsAfter.values()), tick_label=list(countFlairsAfter.keys()), color='orange')\n",
    "pl.title('GME post category January 23rd till 30th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER, textblob sentiment. VADER below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    " \n",
    "def sentiment_classifier(sentence):\n",
    " \n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    " \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # object gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "     \n",
    "    print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "    print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    " \n",
    "    print(\"Sentence Overall Rated As\", end = \" \")\n",
    " \n",
    "    # decide sentiment as positive, negative and neutral\n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        print(\"Positive\")\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        print(\"Negative\")\n",
    " \n",
    "    else :\n",
    "        print(\"Neutral\")\n",
    "        \n",
    "def sentiment_scores(sentence):\n",
    " \n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    " \n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "     \n",
    "    return sentiment_dict['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love the TU/e. It may have it's downsides but generally it's pretty swell.\"\n",
    "\n",
    "sentiment_classifier(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditpostsRelevant = redditPosts.query('link_flair_text == \"Discussion\" or link_flair_text == \"News\" or link_flair_text == \"Loss\" or link_flair_text == \"Gain\"')\n",
    "\n",
    "redditpostsRelevant['text_sentiment_score'] = redditpostsRelevant['title'].apply(sentiment_scores)\n",
    "#takes extremely long, might want to include some kind of progress printing and do it on a pc.\n",
    "redditpostsRelevant\n",
    "\n",
    "#also for now there is only title implementation as selftext might be missing (in the case of memes for example)\n",
    "#but that might actuall©y be an advantage, filtering out memes and removed posts in order to make scoring even more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redditpostsRelevant.to_csv('redditpostsRevelant.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregression test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "\n",
    "lag_plot(prices_hour['GME'])\n",
    "\n",
    "#Clear correlation on just prices, however there needs to be a way to include multile variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
